{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Finding Patterns in Data using IBM Power and PowerAI</h1>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "In this lab we will explore an open source data set, and discover how we can use the tools that are part of **PowerAI** to explore and discover patterns in the data.  For this lab, we will make use of the Lending Club data set, **scikit learn, Tensorflow and Keras**.  Here is a brief description about Lending Club.\n",
    "\n",
    "```\n",
    "About the author's\n",
    "Dustin VanStee - Data Scientist\n",
    "Bob Chesebrough - Data Scientist\n",
    "IBM Cognitive Systems Solution Center\n",
    "contact : vanstee@us.ibm.com\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/techu-banner.png\" width=\"800\" height=\"500\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Lending Club (LC)](https://www.lendingclub.com/) is the world’s largest online marketplace connecting borrowers and investors. It is transforming the banking system to make credit more affordable and investing more rewarding. Lending Club operates at a lower cost than traditional bank lending programs and pass the savings on to borrowers in the form of lower rates and to investors in the form of solid risk-adjusted returns.\n",
    "\n",
    "**The DATA**  \n",
    "The original data set is downloaded from [LC](https://www.lendingclub.com/info/download-data.action) covering complete loan data for all loans issued through the 2007-2018, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. Additional features include credit history, number of finance inquiries, address including zip codes, and state, and collections among others. It is quite rich and is an excellent example of credit risk data.  Interestingly, Goldman Sachs’ new peer-to-peer lending platform called Marcus was built almost entirely using the Lending Club data.\n",
    "\n",
    "Here is a link to some extra information regarding the fields of the data set.\n",
    "[Data Dictionary](https://github.com/dustinvanstee/mldl-101/blob/master/lab5-powerai-lc/LCDataDictionary.csv)\n",
    "\n",
    "**Important**\n",
    "\n",
    "In this notebook, we will play with the lending club data, conduct a set of exploratory analysis and try to apply various machine learning techniques to predict borrower’s default. We took a small sample of loans made in 2016 (130K) to help speed up the processing time for the lab\n",
    "\n",
    "\n",
    "Note : to remove a lot of the busy verbose code, we are making using of a utility python file called lc_utils.py.  For implemenation details you can refer here [python code](https://github.com/dustinvanstee/mldl-101/blob/master/lab5-powerai-lc/lc_utils.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick word on the data science method\n",
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/dsx-methodology.png\" width=\"900\" height=\"700\" align=\"middle\"/>\n",
    "\n",
    "Here we will use these simple high level steps to work through a typical data science problem.  This workflow is meant to be a high level guide, but in practice this is a highly iterative approach ...\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Perform some initial analysis of the data for **Business Understanding**\n",
    "* **Prepare the Data** for our visualization and modeling\n",
    "* **Visualize** the data\n",
    "* Model using **Dimension Reduction** and **Classification** techniques\n",
    "* **Evaluate** the approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Business/Data Understanding and Preparation\n",
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/techu-bu-dp.png\" width=\"800\" height=\"500\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment bootstrapping\n",
    "# !pip install jupyter-pip\n",
    "# !pip3 install brunel\n",
    "# !git fetch origin master\n",
    "# !git reset --hard origin/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code functions that are needed to run this lab\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import glob\n",
    "\n",
    "# custom library for some helper functions \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import myenv as myenv\n",
    "from lc_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "Here we load data that was previously downloaded from lendingclub.com.  For speed of this lab, we are restricting the number of loans ~ 130K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = load_sample_data()\n",
    "loan_df_orig = loan_df\n",
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics (1D)\n",
    "Lets look at some 1D and 2D descriptive statistics for this dataset\n",
    "\n",
    "In this dataset, we have all types of data.  Numerical, Categorical, Ranked data.  This small module will take you through what is typical done to quickly understand the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function provide the number of rows/cols\n",
    "# Information on the types of data\n",
    "# and a report of descriptive statistics\n",
    "\n",
    "quick_overview_1d(loan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can get a quick assessment of the statistics for each column.  \n",
    "**Quick Question** can you answer what was the average income for the 133K loan applicants ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics (2D)\n",
    "Since we have over 100 numerical variables, creating a 2D correlation plot may be time consuming and difficult to interpret.  Lets look at correlations on a smaller scale for now....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab only a subset of columns\n",
    "cols = [\"loan_amnt\",\"annual_inc\",\"dti\",\"fico_range_high\",\"open_acc\",'funded_amnt', 'total_acc']\n",
    "quick_overview_2d(loan_df, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Question** : Can you find a negatively correlated variable to annual_inc in the chart above?  Can you think of a reason for this result ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Loan Default column.  This is the column we will predict later\n",
    "The **loan_status** column contains the information of whether or not the loan is in default. \n",
    "\n",
    "This column has more than just a 'default or paid' status.  Since our goal is to build a simple default classifier , we need to make a new column based off the **loan_status** column.\n",
    "\n",
    "Here we will look at all the categorical values in **loan_status**, and create a new column called **default** based off that one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create loan status .... \n",
    "loan_df = create_loan_default(loan_df)\n",
    "loan_df.head(3) # scroll to the right, and see the new 'default' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Null Values aka NaNs...\n",
    "\n",
    "One part of the data science process thats especially time consuming is working with unclean data.  This lending club data set is a great example of that.  If you look at the dataframe shown above, you will see a number of columns with the indicator **NaN** .  This means 'not a number' and needs to be dealt with prior to any machine learning steps.  You have many options here.  Some options are listed below...\n",
    "\n",
    "* Fill with a value -> impute mean/median/min/max/other\n",
    "* drop rows with NaNs\n",
    "* drop columns with large number of NaNs \n",
    "* use data in other columns to derive\n",
    "\n",
    "All these methods are possible, but its up to the data scientist / domain expert to figure out the best approach.  There is definitely some grey area involved in whats the best approach.\n",
    "\n",
    "First, lets understand which columns have NaNs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every column, count the number of NaNs .... \n",
    "# code hint : uses df.isna().sum()\n",
    "\n",
    "columns_with_nans(loan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, we have some work to do to clean up the NaN values.  Beyond NaN values, we also have to transform columns if they aren't formatted correctly, or maybe we want to transform a column based on custom requirements.  \n",
    "\n",
    "```\n",
    "Example : column=employee_length , values=[1,2,3,4,5,6,7,8,9,10+] formatted as a string\n",
    "          transform into \n",
    "          column=employee_length, [0_3yrs,4_6yrs,gt_6yrs] (categorical:strings)\n",
    "```\n",
    "          \n",
    "Luckily, we took care to process and clean this data below using a few functions.  In practice, **this is where data scientists spend a large portion of their time** as this requires detailed domain knowledge to clean the data.  We have made a fair number of assumptions about how to process the data which we won't go into due to time contraints for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following cleaning of the data makes use of the steps shown below.....\n",
    "\n",
    "#loan_df1 = drop_sparse_numeric_columns(loan_df)\n",
    "#loan_df2 = drop_columns(loan_df1)\n",
    "#loan_df3 = impute_columns(loan_df2)\n",
    "#loan_df4 = handle_employee_length(loan_df3)\n",
    "#loan_df5 = handle_revol_util(loan_df4)\n",
    "#loan_df6 = drop_rows(loan_df5)\n",
    "\n",
    "loan_df = clean_lendingclub_data(loan_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Sanity check ....\n",
    "# If we did our job right, there should not be any NaN's left.  \n",
    "# Use this convenience function to check\n",
    "\n",
    "# code hint df.isna().sum()\n",
    "\n",
    "columns_with_nans(loan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation - Handle Time Objects\n",
    "Sometimes for columns that contain date information, you may want to break them down into individual columns like month, day, day of week etc.  For our use case, we will create a new column called `time_history` that will indicate how long an applicant has been a borrower.  This is an example of **feature engineering**.  Essentially, using business logic to create a new column (feature) that may have predictive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = create_time_features(loan_df)\n",
    "loan_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical Data to One hot encode ###\n",
    "\n",
    "If you look above at the data frame, we are almost ready to start building models.  However, there is one important step to complete.  Notice we have some columns that are still built out of string data \n",
    "```\n",
    "example column=home_ownership values=[RENT, MORTGAGE, OWN]\n",
    "```\n",
    "Machine learning algorithms only process numerical data, so we need to transform these **categorical columns** into **indicator columns**\n",
    "\n",
    "From the example above, the transform would yield 3 new columns\n",
    "\n",
    "```\n",
    "example column=RENT values=[0,1]\n",
    "        column=MORTGAGE values=[0,1]\n",
    "        column=OWN values=[0,1]\n",
    "```\n",
    "\n",
    "Conveniently pandas has a nice function called **get_dummies** that we will use for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform categorical data into binary indicator columns\n",
    "# code hint, uses pd.get_dummies\n",
    "\n",
    "loan_df = one_hot_encode_keep_cols(loan_df)\n",
    "loan_df.head() # once complete, see how many new columns you have!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result after data prep ....\n",
    "\n",
    "Ok, so you made it here, lets take a look at the final results of your data preparation work.  It may be helpful to  **qualitatively compare** your original data frame to this one and see how different they look..  Execute the cells below to get a sense of what the tranformations accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df_orig.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "As you saw, when you 'describe' a data frame, you get a table statistics showing you the mean,min,max and other statistics about each column.  This is good, but sometimes its also good to look at the histograms of the data as well.  Lets Visualize some of the distributions from our dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/data-visualization.png\" width=\"800\" height=\"500\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot distribution charts for all the numerical columns in our dataframe\n",
    "plot_histograms(loan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brunel Example\n",
    "## The Growth of Lending Club\n",
    "### Here we use the builtin Brunel Visualization graphics package\n",
    "Lending club has been expanding over the years in terms of total loan volume and average loan size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a statistics data frame based on issue date\n",
    "# aggregate on loan amount\n",
    "vis_df = loan_df.copy()\n",
    "vis_df['loan_status'] = loan_df_orig['loan_status']\n",
    "loan_stats = pd.concat([vis_df.groupby('issue_d').mean()['loan_amnt'].to_frame().rename(columns = {'loan_amnt':'loan_average'}), vis_df.groupby('issue_d')['loan_status'].count().to_frame().rename(columns = {'loan_status':'loan_count'})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%brunel data('loan_stats') line x(ISSUE_D) y(loan_average, loan_count) color(#series) tooltip(#all) :: width=900, height=350 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Add more Brunel if i get it in nimbix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/modeling.png\" width=\"800\" height=\"500\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test set creation\n",
    "\n",
    "One of the key points in any machine learning workflow is the **partitioning** of the data set into **train** and **test** sets.  The key idea here is that a model is built using the training data, and evaluated using the test data.  \n",
    "\n",
    "There are more nuances to how you partition data into train/test sets, but for purposes of this lab we will omit these finer points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lc_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lendingclub_ml object that will hold our test, and contain methods used for testing.\n",
    "# Implementation done like this to ease the burden on users for keeping track of train/test sets for different\n",
    "# models we are going to build.\n",
    "\n",
    "my_analysis = lendingclub_ml(loan_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train / test split of your data set.  Paramter is test set size percentage \n",
    "# Returns data in the form of dataframes\n",
    "\n",
    "my_analysis.create_train_test(test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "For this modeling exercise we will perform a couple of tasks, **dimension reduction** and **classification** as shown in the following diagram.\n",
    "\n",
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/techu-modeling-workflow.png\" width=\"800\" height=\"500\" align=\"middle\"/>\n",
    "\n",
    "**Dimension Reduction** is useful in scenarios when you have a large number of columns and you would like to reduce that down to a compressed representation .  In this lab we will try 2 methods of dimension reduction.  It will be your choice to decide which method you want to use for the classification part of the lab ! (you could even decice to bypass this if you want ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction - PCA\n",
    "\n",
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/techu-pca.png\"  width=\"200\" height=\"125\" align=\"middle\"/>\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The number of principal components is less than or equal to the number of original variables.\n",
    "\n",
    "A simple way to think about PCA is that it helps compress the data in a lossy representation of the original dataset.\n",
    "\n",
    "This will also be used to help us visualize the data as you will see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension Reduction using PCA\n",
    "my_analysis.build_pca_model(n_components=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the chart above, you can see that we get ok results from PCA.  Using the first 20 principal components, we can account for ~50% of the variance described in the dataset.  **Feel free to change the number of principal components above to see if adding more helps with explained variance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction - AutoEncoder\n",
    "\n",
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/techu-ae.png\"  width=\"600\" height=\"375\" align=\"middle\"/>\n",
    "\n",
    "An autoencoder is another method that can be used for dimension reduction.  An autoencoder is a **neural network** that tries to reproduce itself given the constraint that it will lose information in the bottleneck layer.  Based on this, its again a lossy representation.  One key difference between autoencoders and PCA is that an autoencoder can find non linear relationships between variables that PCA could not detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will build and run your auto-encoder \n",
    "# feel free to adjust \n",
    "# ae_layers -> needs to be an odd number of layers, and symmetric \n",
    "# Regularization -> controls overfitting\n",
    "# epochs -> number of times to loop thru training set\n",
    "\n",
    "my_analysis.build_ae_model(ae_layers=[100,25,6,25,100], regularization=0.001, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Contest**  lets see who can find the best settings for the neural network to minimize loss!  Just yell out your results and instructor will add to the leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now update our test dataframe with new columns that are predicted by our PCA and Autoencoder models.  \n",
    "\n",
    "Here we will now take the models that we built and pass our test data set through the models.   By doing this, we will have reduced the number features in our data set by a significant amount (~177 colums => ~5-20 columns!)  .  \n",
    "\n",
    "In this step we will add new columns to our test/train data frames for both our PCA model and our autoencoder model.  Don't worry about the details of this step, its just required for some followon visualization, and training steps ahead. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_analysis.update_train_test_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool Visualizations using our dimension reduction columns\n",
    "\n",
    "Next we will plot a few scatterplot grids based on our pricipal component and autoencoder representations of the data\n",
    "\n",
    "We will color each data point using this key\n",
    "```\n",
    "Green -> Fully paid or current loan\n",
    "Red   -> Loan in default\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a minute or so ...\n",
    "my_analysis.visualize_dimred_results(mode='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a minute or so ...\n",
    "my_analysis.visualize_dimred_results(mode='ae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can discern a pattern between the red / green dots, its likely we can use a classifier to automatically seperate them! We'll see that in a few more sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap commentary\n",
    "Using a heatmap can be another good visualization tool.  You can use this to get a sense of how the data correlates to each other.  In the code below, play with the **sortColumn** input .  In the example below we are sorting by principal component 0, which has the most information encoded in that column.  See if you can find out what PC0 might be composed of.  Try it for PC1, or AE0, AE1...\n",
    "\n",
    "Pro tip, to get the most out of a heatmap, all the data needs to be normalized on a common 0 -> 1 scale so that the coloring of the columns works out ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a minute or so ...\n",
    "bob_heatmap_lc(my_analysis.test_df,sortColumn='PC0',add_corr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resolution is quite small, but try to find columns that go from solid red on bottom to black on top. That would be an indication of high correclation to your sort column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Step - Lending Club Default Prediction\n",
    "\n",
    "<img src=\"https://github.com/dustinvanstee/random-public-files/raw/master/techu-modeling-traintest.png\"  width=\"600\" height=\"375\" align=\"middle\"/>\n",
    "\n",
    "Here we will build a classifier to predict if loan will fail or not.  We will us a \n",
    "** Deep Learning Classifier **  .  You will have 3 options for data sources, \n",
    "* the raw data\n",
    "* PCA dimension reduction features\n",
    "* Autoencoder features\n",
    "\n",
    "To evaluate our model, we will use a simple contingency table (showing true/false positve/negative).  However, this is a fairly simplistic method.  Better method that data scientists use are F1 score, and PR/ROC curves but thats beyond the scope of this lab.\n",
    "\n",
    "Step 1 here is to set our baseline result.  In this example, we are dealing with a **skewed** dataset.  This means, on average, most people will not default, and they pay their loan off.  If you built a classifier that just predicted no default, you would be right most of the time.  Lets see the stats from our dataset below...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our baseline\n",
    "my_analysis.train_df['default'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, **only ~12.8% of the applicants default**.  Any classifier we build must be better than this, or we aren't doing a very good job ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modes\n",
    "# pca           : principal components only\n",
    "# ae            : autoencoder components only\n",
    "# raw           : all the data non reduced\n",
    "# raw_no_grades : all the data non reduced except the grade info provided by lending club\n",
    "\n",
    "mode = 'pca' # ae , raw, raw_no_grades\n",
    "\n",
    "if(mode == 'pca') :\n",
    "            x_cols = [x for x in my_analysis.train_df.columns if 'PC' in x]\n",
    "elif(mode == 'ae') :\n",
    "            x_cols = [x for x in my_analysis.train_df.columns if 'AE' in x]\n",
    "elif(mode == 'raw') :\n",
    "            x_cols = [x for x in my_analysis.train_df.columns if 'AE' not in x and 'PC' not in x]\n",
    "elif(mode == 'raw_no_grades') :\n",
    "            x_cols = [x for x in my_analysis.train_df.columns if 'AE' not in x and 'PC' not in x]\n",
    "            import re\n",
    "            x_cols = [x for x in x_cols if not re.match('^[ABCDEFG]',x)]\n",
    "\n",
    "#print(x_cols)\n",
    "my_analysis.build_evaluate_dl_classifier(x_cols, epochs=25,batch_size=32,regularization=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Contest** See who can get the best accuracy, shout out your answers to the instructor and see if you can top the leaderboard\n",
    "* accuracy = (true positive + true negative) / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Credits \n",
    "* Bob Chesebrough - CSSC major contributor\n",
    "* [Hands on Machine Learning - Geron] (https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)\n",
    "\n",
    "### More Learning\n",
    "* Coursera Deeplearning.ai  (Ng)\n",
    "* Coursera Machine Learning (Ng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
