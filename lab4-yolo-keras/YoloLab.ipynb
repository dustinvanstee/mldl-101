{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection - YOLO Lab using PowerAI\n",
    "\n",
    "The goal of the notebook is to introduce a few key concepts around YOLO, and show how you can use Keras and Tensorflow which are part of the PowerAI distribution to solve these types of problems.\n",
    "\n",
    "Many of the ideas in this notebook are described in the two YOLO papers: Redmon et al., 2016 (https://arxiv.org/abs/1506.02640) and Redmon and Farhadi, 2016 (https://arxiv.org/abs/1612.08242). \n",
    "\n",
    "**You will learn to**:\n",
    "- Use object detection to \n",
    "    - draw bounding boxes on images\n",
    "    - count object instances\n",
    "- Create a video with bounding boxes\n",
    "- [Optional - Work In Progress]: Training a network using a new dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Applications of Object Detection ....\n",
    "\n",
    "<img src=\"nb_images/applications.png\" style=\"width:700px;height:400;\">\n",
    "<caption><center> <u> **Figure 2** </u>: **Applications for Object Detection with YOLO**<br> </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions :\n",
    "- PowerAI - IBM Deeplearning Frameworks optimized on IBM Power Systems leveraging Nvidia GPUs\n",
    "- YOLO   -  object detection algorithm using state of the art deep learning technique\n",
    "- COCO Dataset - a standard object detection dataset hosting > 100K labelled images\n",
    "- Tensor - An N dimensional array  \n",
    "- Training - Phase of machine/deep learning where you are adjusting model weights to predict known outputs.\n",
    "- Inference - Phase of machine/deep learning where you are using a previously trained model to predict output with new data\n",
    "- Anchor Boxes - Technique used in object detection algorithms to allow detection of more than one object in a grid cell\n",
    "\n",
    "- non max suppression - Technique used is object detection algorithm to prune away low probability predicted boxes.\n",
    "- batch norm - Technique use in deep learning to allow for faster training.\n",
    "- leaky relu - Non linear activation function that is used in many image classification / detection algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following cell to load the packages and dependencies and lets get started!\n",
    "import argparse as ap\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Lambda, Conv2D\n",
    "from keras.models import load_model, Model, model_from_json\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "\n",
    "from yolo_utils import read_classes, read_anchors\n",
    "import random\n",
    "import colorsys\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load library functions\n",
    "Most of the code for this lab is provided via the `yolo_demo` convenience library.  This code is available on github at \n",
    "- https://github.com/dustinvanstee/mldl-101/tree/master/lab4-yolo-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all the helper functions that we will use for this lab\n",
    "from yolo_demo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: As you can see, we import Keras's backend as K. This means that to use a Keras function in this notebook, you will need to write: `K.function(...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove this ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(os.environ['PYTHONPATH'])\n",
    "print(tf.__version__)\n",
    "print(tf.__path__)\n",
    "#import keras\n",
    "#print(keras.__version__)\n",
    "#print(keras.__path__)\n",
    "print(sys.version)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab explanation\n",
    "\n",
    "Here we use a Keras implementation built by Allan Zelener called Yad2k .  This initial implementation has been significantly modified for this lab. \n",
    "\n",
    "**Under the hood, we will leverage PowerAI Keras/Tensorflow libraries**\n",
    "\n",
    "Below we will load our demo environment.  \n",
    "\n",
    "The general flow of this lab will be to expose some high level functions to see how the algorith works in general, but the complexity of the code will be encapsulated in the yolo_demo.py .  All this code is accessible at \n",
    "https://github.com/dustinvanstee/mldl-101/tree/master/lab4-yolo-keras\n",
    "\n",
    "This lab will make use of a pre-trained yolo model that was built using the COCO dataset.  *This lab will not implement retraining this network based on newly labelled images and classes.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Session \n",
    "\n",
    "This lab will make use of both Keras and Tensorflow.  Keras is used for the high level implementation of the neural network, and tensorflow is used as the deep learning \"backend\".  Note, in many cases it is ok to mix tensorflow and keras code, but in general, for portability use Keras as much as possible.\n",
    "\n",
    "Here we load a new tensorflow session via K.get_session() and setup the yolo_demo configuration files.  \n",
    "\n",
    "The configurations files define the 80 coco classes, and the anchor box coordinates used for this demo.\n",
    "- Note : anchor boxes are used when there are multiple objects in the same grid location.\n",
    "\n",
    "The mydemo object has all the built in functions we will need.  Lets explore what we can do  ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = K.get_session()\n",
    "\n",
    "# TF Debugger... try standard!\n",
    "#from tensorflow.python import debug as tf_debug\n",
    "#sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "#K.set_session(sess)\n",
    "\n",
    "mydemo = yolo_demo(sess,batch_size=1) \n",
    "mydemo.vi_mode(\"image\")\n",
    "mydemo.class_names(read_classes(\"./model_data/coco_classes.txt\"))\n",
    "mydemo.anchors(read_anchors(\"./model_data/yolo_anchors.txt\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Classes in the COCO dataset\n",
    "Here we are using a model that was pre-trained on the MS COCO dataset \n",
    "http://cocodataset.org/#home\n",
    "\n",
    "Lets take a look at the 80 classes we can detect by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(mydemo.class_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image into Demo\n",
    "\n",
    "For this lab, lets start with an image pre-downloaded from the web just to experiment.  We will try to identify all the elephants and giraphes in this image.  \n",
    "** note : this a much more difficult task than basic image classification **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydemo.input_vi(\"./images/safari2.jpg\")\n",
    "mydemo.set_image_shape()\n",
    "mydemo.infer_mode(\"darknet\")\n",
    "Image(\"images/safari2.jpg\")\n",
    "# note the number of elephants and giraphes for later... (7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Pre-trained Model\n",
    "Here we make use of a model that was built using the Coco dataset.  The h5 file is a standard format for DL models.\n",
    "\n",
    "HDF5 file will typically contain:\n",
    "\n",
    "* the architecture of the model, allowing to re-create the model\n",
    "* the weights of the model\n",
    "* the training configuration (loss, optimizer)\n",
    "* the state of the optimizer, allowing to resume training exactly where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = !uname -m\n",
    "system = str(system[0])\n",
    "print(\"System type = {}\".format(system))\n",
    "if(system == 'x86_64' and not os.path.exists('./model_data/yolo.h5')) :\n",
    "    print(\"bypassing download\")\n",
    "elif(system == 'ppc64le' and not os.path.exists('./model_data/yolo.h5')) :\n",
    "    !wget https://github.com/dustinvanstee/mldl-101/releases/download/v1.0/yolo_power.h5 -O model_data/yolo.h5\n",
    "else :\n",
    "    print(\"already have model_data/yolo.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load YOLO model\n",
    "\n",
    "Lets use the convenience function : `load_and_build_graph` to create the static graph that will be used to evaluate images\n",
    "\n",
    "`load_and_build_graph` performs the following function\n",
    "- load keras model and apply pre-trained weights\n",
    "- add transformation function to convert m x 19 x 19 x 85 array to probabilities and boxes\n",
    "- filtering functions to keep only the high probability boxes (uses non max suppression technique)\n",
    "\n",
    "Model loading is done in keras using the function \n",
    "\n",
    "`yolo_model = keras.models.load_model(pretrained_model)`\n",
    "\n",
    "Lets look at this drawing to see whats going on under the hood \n",
    "\n",
    "<img src=\"nb_images/architecture.png\" style=\"width:700px;height:400;\">\n",
    "<caption><center> <u> **Figure 2** </u>: **Inference/Encoding architecture for YOLO**<br> </center></caption>\n",
    "\n",
    "\n",
    "This function instantiates the YOLO network that was pretrained on the COCO dataset.  \n",
    "\n",
    "Initially you start with an image.  This image can be rectangular.  Here is the inference pipeline steps\n",
    "- Resize image to a square image, eg 608 x 608 pixels\n",
    "- Then feed image into YOLO CNN network\n",
    "- Output of this network is a grid of outputs.  At each grid location, you will have values track (Pc, x,y, w, h, one hot class vector) .  Repeat this structure for each anchor box\n",
    "- Next take this output and prune boxes down to high probalility boxes using non max suppression algorithm\n",
    "- List of boxes can be shown on image or fed into downstream system for counting or other applications\n",
    "\n",
    "**note : We will omit the training discussion of YOLO due to complexity and scope. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_and_build_graph\n",
    "# Params :\n",
    "# score_threshold : Modify the score_threshold to be more certain about which boxes are displayed.\n",
    "# iou_threshold :   Reduce to iou_threshold to allow more boxes that have some overlap (higher value is less pruning since it will allow more overlap)\n",
    "\n",
    "mydemo.load_and_build_graph(arch=\"default\",weights=\"default\", max_boxes=15, score_threshold=0.5, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Model Summary\n",
    "\n",
    "Now that we have loaded the model, Keras provides a great summary functions so that we can view all the model layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydemo.print_model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Description\n",
    "\n",
    "If you look at the output above, you can see the large number of layers used to create the model.  Explanations of the functions of the layers are in the definitions section at the top.   Next to the name of each layer is a value in parentheses like `(None, 19, 19, 425)`.  This indicates the size of the output Tensor.  Then `None` keyword is the idiomatic way Keras and TF use to identify the dimension that includes the batch size.  `None` is specified because batch size is determined at runtime, and so you cant define that dimension during static initialization of the graph.\n",
    "\n",
    "Note at the bottom the number of parameters in the model.  Keras provides methods for freezing / unfreezing layers of the graph.  This is important during training, but not during inference.\n",
    "\n",
    "Over 50 Million parameters are tuned during the training phase of the model.  A computationally intensive task that requires GPU to accelerate!  *** This is what the GPU enabled Power8 and Power9 servers are built for ...***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Inference on a single image\n",
    "\n",
    "First lets use an image from the pre-populated images directory.  In this image we will identify elephants and giraphes.\n",
    "\n",
    "The `process_image` convenience function is provided to \n",
    "- read in the image into numpy array using OpenCV\n",
    "- Scale the image to a 608 x 608 image using OpenCV\n",
    "- Perform inference on the image\n",
    "- Prune discovered boxes down using values specified in the `load_and_build_graph` function call\n",
    "- Draw bounding boxes on the image\n",
    "- Save the new image to the `output_image` directory\n",
    "\n",
    "Run the cell below, and see the description below for an interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydemo.input_vi(\"./images/wine-glass-sizes.jpg\")\n",
    "#mydemo.set_image_shape()\n",
    "#mydemo.process_image(output_image=\"images/wine_boxes.jpg\")\n",
    "mydemo.process_image(output_image=\"images/safari_boxes.jpg\", with_grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpret the results above ....\n",
    "The lines that start with **`draw_boxes`** give details about the discovered boxes including\n",
    "- class index\n",
    "- class label\n",
    "- probablity the object is the specified class\n",
    "- bounding box coordinates\n",
    "\n",
    "\n",
    "Sample debug output looks like this<br>\n",
    "<pre>\n",
    "                 class index   label,         BoxProb,upper left coord,     lower right coord\n",
    "draw_boxes : 14  classIdx=20   label=elephant 0.17    xmin,ymin=(621, 461)  xmax,ymax=(730, 491)\n",
    "</pre>\n",
    "\n",
    "The dotted 19 x 19 grid is debug output to show the most likely class at each grid location. \n",
    "Cells with 20 indicate a likelihood of an elephant\n",
    "Cells with 23 indicate a likelihood of an giraffe\n",
    "\n",
    "This view is really an intermediate step you can use to understand more of what is going at each grid location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Labeled Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/safari_boxes.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Results\n",
    "\n",
    "*Do you notice how the 3 giraffes in the middle of the image are not highlighted?*\n",
    "*Did you notice we only detected 2 giraphes and 5 elephants even though we should have detected 7 for both classes?*\n",
    "\n",
    "First try this experiment, lets re-run the sequence above with a couple of different settings.  We will modify : \n",
    "- score_threshold from 05 -> 0.15\n",
    "- iou_threshold from 0.5 -> 0.7\n",
    "\n",
    "The score_threshold will reduce the required probabiliy for something to be considered detected.\n",
    "The iou_threshold increase will reduce the pruning of overlapping boxes.  \n",
    "\n",
    "To make this easier, a convenience function called infer image is used .  Essentially this encapsulates everything we did above into one inference function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_image(input_image=\"./images/safari2.jpg\",\n",
    "            output_image=\"./images/safari_boxes.jpg\",\n",
    "            score_threshold=0.15,\n",
    "            iou_threshold=0.7,\n",
    "            max_boxes=15,\n",
    "            audit_mode=True,\n",
    "            tfdbg=False,\n",
    "            mode=\"darknet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"images/safari_boxes.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of new image results\n",
    "\n",
    "When score threshold is changed from 0.5 to 0.15 we pick up the giraffes in the middle.  And the iou_threshold increase allows us to keep more boxes even though we have overlaps.  Still not a perfect result, but a lot better than before.  **Feel free to try a couple of different settings here to get a sense of how the knobs work.**\n",
    "\n",
    "For images with a high concentration of objects, these knobs require some tuning to get the best results.  Also training with more images that represent this distribution of pictures would help quite a bit too.  Not bad for a general purpose model though.\n",
    "\n",
    "\n",
    "\n",
    "Next, lets try this with a sample image downloaded from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional ]: Download image from the web, and perform inference\n",
    "\n",
    "Use google and search for images of something below....\n",
    "\n",
    "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "Once you find a jpg, copy the link using ** copy link address ** and then paste the link after the wget command below.\n",
    "\n",
    "Then run the wget command below to save into the images directory (just leave the output file name as test.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Change the code below ###\n",
    "# this one was a fun image .. https://cdn.shopify.com/s/files/1/2298/4179/products/20-5630_FruitBowlFilled.jpg?v=1507743561\n",
    "\n",
    "!wget 'type your url here' -O images/test.jpg\n",
    "#### Dont change below this line ####\n",
    "\n",
    "# Verify the image is now in the images directory\n",
    "!ls images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Dont need to change this code ####\n",
    "input_image =\"images/test.jpg\"\n",
    "\n",
    "\n",
    "output_file = \"./images/optional_test.jpg\"\n",
    "infer_image(input_image=input_image,\n",
    "            output_image=output_file,\n",
    "            score_threshold=0.15,\n",
    "            iou_threshold=0.7,\n",
    "            max_boxes=15,\n",
    "            audit_mode=True,\n",
    "            tfdbg=False,\n",
    "            mode=\"darknet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label a video with bounding boxes\n",
    "\n",
    "Here we make use of everything we have just done above, but now we will process a video which is jsut a series of images.  You can think of it like adding an outer loop to processing an image.\n",
    "\n",
    "Here we use openCV to read portions of a video into a numpy array, and have a few settings to control the # of frames processed.  Also for time purposes, we skip a couple of frames just to make the processing go quicker.  Run the function below to see the end result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_video(input_video=\"./sampleVideos/ElephantStampede2.mov\",\n",
    "            audit_mode=False,\n",
    "            output_dir=\"./output\",\n",
    "            mode=\"darknet\",\n",
    "            batch_size=8,\n",
    "            frame_stride=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Workaround to convert avi file to mov... Need this to display movie in notebook for now\n",
    "!ffmpeg  -y -loglevel warning -i output/processed_video.avi  ./output/processed_video.mov\n",
    "!ls output # should see ./output/processed_video.mov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the labelled video\n",
    "\n",
    "<center>\n",
    "<video width=\"800\" height=\"400\" src=\"./output/processed_video.mov\" type=\"video/mov\" controls>\n",
    "</video>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "<font color='blue'>\n",
    "**What you should remember**:\n",
    "- YOLO is a state-of-the-art object detection model that is fast and accurate\n",
    "- It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume. \n",
    "- The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.\n",
    "- You filter through all the boxes using non-max suppression. Specifically: \n",
    "    - Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes\n",
    "    - Intersection over Union (IoU) thresholding to eliminate overlapping boxes\n",
    "- Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise. \n",
    "\n",
    "<font color='black'>\n",
    "\n",
    "**References**: The ideas presented in this notebook came primarily from the two YOLO papers. The implementation here also took significant inspiration and used many components from Allan Zelener's github repository. The pretrained weights used in this exercise came from the official YOLO website. \n",
    "- Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) (2015)\n",
    "- Joseph Redmon, Ali Farhadi - [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) (2016)\n",
    "- Allan Zelener - [YAD2K: Yet Another Darknet 2 Keras](https://github.com/allanzelener/YAD2K)\n",
    "- The official YOLO website (https://pjreddie.com/darknet/yolo/) \n",
    "- Coursera.com / deeplearning.ai - (YOLO lectures and Lab)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OMdut",
   "launcher_item_id": "bbBOL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
